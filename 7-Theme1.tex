\Chapter{DÉTAILS DE LA SOLUTION}\label{sec:Theme1}

Cette partie du mémoire est consacrée à la résolution analytique des équations associées aux différents objectifs formulés en introduction (\ref{sec:Introduction}).

\section{Diffusion pure}
D'abord, le processus \acs{CIR} sous forme de diffusion pure est considéré. 
\subsection{Fonction Génératrice des Moments}\label{subsection_fgm_eq}
\paragraph{Dérivation de l'équation à résoudre}\phantom{}\\
Soit un processus d'Itô défini par l'\acs{EDS} (\ref{ito_eq}).
Il est connu que la \acl{FGM} d'un temps de premier passage $\tau(x)$ satisfait l'équation du passé de Kolmogorov (voir, par exemple,\cite{cox2017} ou\cite{lefebvre2007}): 
\[
\frac{1}{2}\sigma{(t,x)}^2M''(x;\alpha)+\mu(t,x)M'(x;\alpha)-\alpha M(x;\alpha)=0
\]
avec $M(0;\alpha)=M(c;\alpha)=1$. 

En reprenant les termes de dérive et de diffusion du \acs{CIR} (\ref{cir_eq}), l'\acs{EDO} linéaire de second ordre à résoudre devient: 
\begin{equation}\label{ode_fgm}
    \frac{1}{2}\sigma^2xM''(x;\alpha)+a(b-x)M'(x;\alpha)-\alpha M(x;\alpha)=0
\end{equation}

\paragraph{Résolution}\phantom{}\\
D'abord, en multipliant les deux côtés par $2/\sigma^2$ l'équation ci-dessus (\ref{ode_fgm}) est mise sous forme canonique: 
\[
xM''(x;\alpha)-\left(\frac{2a}{\sigma^2}x-\frac{2ab}{\sigma^2}\right)M'(x;\alpha)-\frac{2\alpha}{\sigma^2} M(x;\alpha)=0
\]
Ensuite, un changement de variable $y=\beta x$ avec $M(x;\alpha)=G(y;\alpha)$ est introduit. L'équation devient: 
\[
yG''(y;\alpha)-\left(\frac{2a}{\sigma^2} \beta y - \frac{2ab}{\sigma^2}\right)G'(y;\alpha)-\beta\frac{2\alpha}{\sigma^2} G(y;\alpha) = 0
\]
Ce changement de variable a pour objectif de déterminer, en fonction des autres paramètres du problème, la valeur de $\beta$ qui permet de ramener l'équation à la forme générale de l'équation de Kummer (\ref{kummer_eq}): 
\begin{equation}\label{kummer_eq}
    xf''(x)-(x-\theta)f'(x)-s f(x)=0,\quad\quad \theta,s\in\mathds{R}
\end{equation}
dont la solution est connue. Pour ce faire, il faut que:
\[
\frac{2a}{\sigma^2} \beta=1\implies\beta=\frac{\sigma^2}{2a}
\]
L'équation devient:
\begin{equation}\label{kummer_eq2}
    yG''(y;\alpha)-\left(y-\frac{2ab}{\sigma^2}\right)G'(y;\alpha)-\frac{\alpha}{a}G(y;\alpha) = 0
\end{equation}
La solution générale de cette dernière (\ref{kummer_eq2}) est de la forme (voir\cite{magnus1966}): 
\[G(y;\alpha) = C_1\Phi\left(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, y\right) + C_2\Psi\left(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, y\right)\]
avec $C_1$ et $C_2$ des constantes à déterminer, et $\Phi(\cdot, \cdot, \cdot)$ et $\Psi(\cdot, \cdot, \cdot)$ sont les fonctions hypergéométriques confluentes de première et seconde espèce (voir annexe \ref{special_functions}).

Finalement, l'expression analytique de la \acs{FGM} de $\tau(x)$ est: 
\begin{equation}\label{sol_fgm}
    M(x;\alpha) = C_1\Phi\left(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, \frac{2ax}{\sigma^2}\right) + C_2\Psi\left(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, \frac{2ax}{\sigma^2}\right)
\end{equation}

\paragraph{Détermination des constantes}\phantom{}\\
Les conditions aux limites $M(0;\alpha)=M(c;\alpha)=1$ permettent de déterminer les deux constantes $C_1$ et $C_2$ en résolvant le système suivant:
\[
\begin{cases}
    C_1\Phi\left(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, 0\right) + C_2\Psi\left(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, 0\right) = 1 \\
C_1\Phi\left(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, \frac{2ac}{\sigma^2}\right) + C_2\Psi\left(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, \frac{2ac}{\sigma^2}\right) = 1
\end{cases}
\]
Il en découle les valeurs suivantes pour $C_1$ et $C_2$: 
\begin{equation}\label{fgm_constants}
    \begin{aligned}
        &C_1 = \frac{\Phi(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, 0)-\Psi(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, 0)}{\Phi(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, \frac{2ac}{\sigma^2})\Psi(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, 0)-\Psi(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, \frac{2ac}{\sigma^2})} \\
        &C_2 = \frac{\Phi(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, \frac{2ac}{\sigma^2})-1}{\Phi(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, \frac{2ac}{\sigma^2})\Psi(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, 0)-\Psi(\frac{\alpha}{a}, \frac{2ab}{\sigma^2}, \frac{2ac}{\sigma^2})}
    \end{aligned}
\end{equation}

\subsection{Fonction Temps Moyen}\label{subsection_mean_eq}

\paragraph{Dérivation de l'équation à résoudre}\phantom{}\\
En combinant le développement en série entière de l'exponentielle: 
\[
e^x=\sum_{k=0}^\infty\frac{x^k}{k!}
\]
et la définition de la \acl{FGM} (\ref{fgm}), il est possible d'écrire (en supposant que les moments de $\tau(x)$ existent et sont finis): 
\begin{equation}\label{taylor_fgm}
    \begin{aligned}
        M(x;\alpha)=\mathds{E}\left[e^{-\alpha \tau(x)}\right]&=\mathds{E}\left[\sum_{k=0}^\infty\frac{{(-\alpha\tau(x))}^k}{k!}\right] \\
        &= \sum_{k=0}^\infty\frac{{(-\alpha)}^k\mathds{E}\left[{\tau(x)}^k\right]}{k!}\\
        &=1-\alpha\mathds{E}[\tau(x)]+\frac{\alpha^2}{2}\mathds{E}\left[{\tau(x)}^2\right]-\ldots
    \end{aligned}
\end{equation}
En injectant (\ref{taylor_fgm}) dans l'équation (\ref{ode_fgm}), et en reprenant la définition (\ref{mean}), il découle l'\acs{EDO} linéaire de second ordre suivante: 
\begin{equation}\label{ode_mean}
    \frac{1}{2}\sigma^2xm''(x)+a(b-x)m'(x)=-1   
\end{equation}
avec $m(0)=m(c)=0$. En résolvant cette équation, une expression analytique de la fonction temps moyen est obtenue. 

\paragraph{Réduction d'ordre}\phantom{}\\
D'abord, afin d'alléger la notation, soit:
\begin{equation}\label{notation}
\begin{cases}
    \alpha = \frac{2a}{\sigma^2} \\
    \beta=\frac{2ab}{\sigma^2} \\
    \theta=-\frac{2}{\sigma^2}
\end{cases}
\end{equation}
L'équation peut donc être réécrite comme suit: 
\begin{equation}\label{simplified_ode_mean}
    xm''(x)+(\beta-\alpha x)m'(x)=\theta
\end{equation}
La réduction d'ordre $u(x)=m'(x)$ donne l'\acs{EDO} linéaire de premier ordre suivante:
\begin{equation}\label{order_reduction}
xu'(x)+(\beta-\alpha x)u(x)=\theta
\end{equation}
Pour procéder à sa résolution, il faut résoudre l'équation homogène associée puis déduire une solution particulière avec la méthode de variation de la constante. La solution générale obtenue est, avec $C_1$ une constante: 
\begin{equation}\label{sol_first_order_mean}
    u(x)=x^{-\beta}e^{\alpha x}(C_1-\theta \alpha^{-\beta}\Gamma(\beta, \alpha x))
\end{equation}
où $\Gamma(\cdot,\cdot)$ est la fonction Gamma incomplète (\ref{special_functions}).
Il suffit donc d'intégrer $u(x)$ et d'ajouter une constante $C_2$ pour obtenir l'expression de $m(x)$: 
\begin{equation}\label{intergration_sol_first_order}
    m(x)=\int u(x)dx+C_2
\end{equation}
Cependant, un problème survient lors de l'intégration du terme:
\begin{equation}\label{problematic_integral}
    \int\theta \underbrace{\phantom{|}{(\alpha x)}^{-\beta}\phantom{|}}_{P}\underbrace{\phantom{|}e^{\alpha x}\phantom{|}}_{E}\underbrace{\phantom{|}\Gamma(\beta, \alpha x)\phantom{|}}_{G}dx
\end{equation}
dans l'expression de $u(x)$ donnée par (\ref{sol_first_order_mean}). En effet, cette intégrale ne possède pas de solution analytique. Les logiciels de calcul symbolique tels que \textit{Mathematica} ou \textit{Maple} échouent également à en trouver une. Il est donc nécessaire d'explorer une autre approche afin de contourner cette difficulté.
\paragraph{Intégration}\phantom{}\\
L'intégrande de (\ref{problematic_integral}) présente des difficultés en raison de la présence du terme puissance $P:={(\alpha x)}^{-\beta}$ multiplié par le terme $E:=e^{\alpha x}$, ainsi que le terme $G:=\Gamma(\beta, \alpha x)$ (lui-même une intégrale). L'objectif est donc de reformuler cette expression afin de simplifier ou d'éliminer certains termes problématiques. C'est précisément ce qui sera abordé dans la suite de cette section. En effet, en combinant les deux expressions suivantes (voir\cite{NIST:DLMF}): 
\begin{align*}
    \left\{
    \begin{aligned}
        \Gamma(s,x) &= \Gamma(s)-\gamma(s,x) \\
        \gamma(s,x) &= x^s\Gamma(s)e^{-x}\sum_{k=0}^{\infty} \frac{x^k}{\Gamma(s+k+1)}
    \end{aligned}
    \right.
\end{align*}
avec $\gamma(\cdot,\cdot)$ une autre forme de la fonction gamma incomplète (voir annexe \ref{special_functions}). 

Il est possible d'écrire:
\[
    \Gamma(s,x) = \Gamma(s)-x^s\Gamma(s)e^{-x}\sum_{k=0}^{\infty} \frac{x^k}{\Gamma(s+k+1)}
\]
et donc, en reprenant les termes de l'équation à résoudre (\ref{notation}) il découle l'expression suivante: 
\begin{equation}\label{incomplete_gamma_simplification}
    \Gamma(\beta, \alpha x) = \Gamma(\beta)-\Gamma(\beta)\underbrace{\phantom{|}{(\alpha x)}^\beta\phantom{|}}_{P'} \underbrace{\phantom{|}e^{-\alpha x}\phantom{|}}_{E'}\sum_{k=0}^{\infty} \frac{{(\alpha x)}^k}{\Gamma(\beta+k+1)}
\end{equation}
L'expression ci-dessus (\ref{incomplete_gamma_simplification}) est très intéressante. En effet, en remplaçant le terme $G$ dans (\ref{problematic_integral}) par (\ref{incomplete_gamma_simplification}), les termes $P$ et $P'$ ainsi que $E$ et $E'$ se simplifient comme suit:
\[
    \begin{aligned}
        &\int\theta \underbrace{\phantom{|}{(\alpha x)}^{-\beta}\phantom{|}}_{P}\underbrace{\phantom{|}e^{\alpha x}\phantom{|}}_{E}\overbrace{\left(\Gamma(\beta)-\Gamma(\beta)\underbrace{\phantom{|}{(\alpha x)}^\beta\phantom{|}}_{P'} \underbrace{\phantom{|}e^{-\alpha x}\phantom{|}}_{E'}\sum_{k=0}^{\infty} \frac{{(\alpha x)}^k}{\Gamma(\beta+k+1)}\right)}^{G}dx \\
        &= \int\theta\Gamma(\beta)\underbrace{\phantom{|}{(\alpha x)}^{-\beta}\phantom{|}}_{P}\underbrace{\phantom{|}e^{\alpha x}\phantom{|}}_{E}dx-\int\theta\Gamma(\beta)\underbrace{\phantom{|}{(\alpha x)}^{-\beta}\phantom{|}}_{P}\underbrace{\phantom{|}{(\alpha x)}^\beta\phantom{|}}_{P'}\underbrace{\phantom{|}e^{\alpha x}\phantom{|}}_{E} \underbrace{\phantom{|}e^{-\alpha x}\phantom{|}}_{E'}\sum_{k=0}^{\infty} \frac{{(\alpha x)}^k}{\Gamma(\beta+k+1)} \\
        &= \int\theta\Gamma(\beta)\underbrace{\phantom{|}{(\alpha x)}^{-\beta}\phantom{|}}_{P}\underbrace{\phantom{|}e^{\alpha x}\phantom{|}}_{E}dx-\theta\Gamma(\beta)\int\sum_{k=0}^{\infty} \frac{{(\alpha x)}^k}{\Gamma(\beta+k+1)}
    \end{aligned}
\]
En injectant dans (\ref{intergration_sol_first_order}), il découle: 
\[
m(x)=(C_1-\theta\alpha^{-\beta}\Gamma(\beta))\underbrace{\int x^{-\beta}e^{\alpha x}dx}_I + \,\theta\Gamma(\beta) \underbrace{\int\sum_{k=0}^{\infty} \frac{{(\alpha x)}^k}{\Gamma(\beta+k+1)}dx}_J + C_2
\]
avec $I$ et $J$ deux intégrales à résoudre: 
\begin{itemize}
    \item \textbf{Résolution de} $I$: \\
    \textit{Wolfram Mathematica} donne:
    \begin{equation}\label{exponential_integral}
        \int x^{-\beta}e^{\alpha x}dx = -x^{1-\beta}E_\beta(-\alpha x)
    \end{equation}
    où $E_n(x)$ est la fonction intégrale exponentielle généralisée (voir annexe \ref{special_functions}).
    La relation suivante (voir\cite{abramowitz1964}):
    \[
    E_n(x)=x^{n-1}\Gamma(1-n,x)
    \]
    permet d'écrire: 
    \begin{equation}\label{exponential_integral_simplification}
        E_{\beta}(-\alpha x)={(-\alpha x)}^{\beta-1}\Gamma(1-\beta,-\alpha x)
    \end{equation}
    En combinant (\ref{exponential_integral}) et (\ref{exponential_integral_simplification}), l'expression analytique de la solution de $I$ est obtenue:
    \[
    \begin{aligned}
        I=\int x^{-\beta}e^{\alpha x}dx&=-x^{1-\beta}E_\beta(-\alpha x)\\
        &= -x^{1-\beta}{(-\alpha x)}^{\beta-1}\Gamma(1-\beta,-\alpha x)\\
        &=-{(-\alpha)}^{\beta-1}\Gamma(1-\beta,-\alpha x)
    \end{aligned}
    \]
    \item \textbf{Résolution de $J$}: \\
    La série à l'intérieur de l'intégrale converge uniformément grâce au terme factoriel au dénominateur. L'intégration terme-à-terme est donc possible: 
    \[
    \begin{aligned}
        \int\sum_{k=0}^{\infty} \frac{{(\alpha x)}^k}{\Gamma(\beta+k+1)}dx &= \sum_{k=0}^{\infty} \int \frac{{(\alpha x)}^k}{\Gamma(\beta+k+1)}dx \\
        &= \sum_{k=0}^{\infty} \frac{\alpha^k x^{k+1}}{(k+1)\Gamma(\beta+k+1)} \\
        &= \frac{x}{\Gamma(1+\beta)}\,_2F_2\left(\begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}2\\1+\beta\end{bmatrix},\alpha x\right)
    \end{aligned}
    \]
    où $_p F_q(\cdot,\cdot,\cdot)$ est la fonction hypergéométrique généralisée (voir annexe \ref{special_functions}).
    
\end{itemize}
La forme finale de l'expression de la fonction temps moyen est donc:
\begin{equation}\label{sol_mean}
    m(x)={(-\alpha)}^{\beta-1}\Gamma(1-\beta,-\alpha x)\left[\theta\alpha^{-\beta}\Gamma(\beta)-C_1\right]+\frac{\theta x}{\beta}\,{}_2F_2\left(\begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}2\\1+\beta\end{bmatrix},\alpha x\right) + C_2
\end{equation}

\paragraph{Détermination des constantes}\phantom{}\\
Les conditions aux limites $m(0)=m(c)=0$ permettent de déterminer les deux constantes $C_1$ et $C_2$ en résolvant le système suivant: 
\begin{align*}
\left\{\begin{aligned}
(\theta\alpha^{-\beta}\Gamma(\beta)-C_1)\alpha^{\beta-1}\Gamma(1-\beta) + C_2 &= 0\\
(\theta\alpha^{-\beta}\Gamma(\beta)-C_1)\alpha^{\beta-1}\Gamma(1-\beta,\alpha c) + \frac{c\theta}{\beta}\,{}_2F_2\left(\begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}2\\1+\beta\end{bmatrix},\alpha c\right)+ C_2 &= 0
\end{aligned}\right. \\
\end{align*}
Les expressions des constantes $C_1$ et $C_2$ sont donc:
\begin{equation}\label{mean_constants}
    \begin{aligned}
        C_1 &= \alpha^{-\beta}\theta\Gamma(\beta)+\frac{\alpha c \theta {(-\alpha)}^{-\beta}}{\beta\gamma(1-\beta,\alpha c)}\,{}_2F_2\left(\begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}2\\1+\beta\end{bmatrix},\alpha c\right) \\
        C_2 &= -\frac{c\theta\Gamma(1-\beta)}{\beta\gamma(1-\beta,\alpha c)}\,{}_2F_2\left(\begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}2\\1+\beta\end{bmatrix},\alpha c\right)
    \end{aligned}
\end{equation}
avec $\alpha$, $\beta$ et $\theta$ définis en (\ref{notation}).

\subsection{Fonction Aire Moyenne}

\paragraph{Dérivation de l'équation à résoudre}\phantom{}\\
Il est connu que l'\acs{EDO} de second ordre régissant la fonction (\ref{area}) est (voir\cite{abundo2013}): 
\[\frac{1}{2}\sigma^2xA''(x)+a(b-x)A'(x)=-x\]
En reprenant les notations introduites en (\ref{notation}), l'équation devient: 
\[xA''(x)+(\beta-\alpha x)A'(x)=\theta x\]
Il convient de noter que cette équation ressemble beaucoup à celle dérivée en (\ref{simplified_ode_mean}). La résolution se fera donc de manière semblable.
\paragraph{Réduction d'ordre}\phantom{}\\
En procédant de façon identique à la résolution de l'équation du temps moyen (\ref{order_reduction},\ref{sol_first_order_mean},\ref{intergration_sol_first_order}), il est possible d'écrire: 
\[xu'(x)+(\beta-\alpha x)u(x)=\theta x\]
et donc: 
\[u(x)=C_1x^{-\beta} e^{\alpha x} -\theta\alpha^{-\beta-1}x^{-\beta} e^{\alpha x}\Gamma(\beta+1, \alpha x)\]
En combinant l'expression dérivée précédemment (\ref{incomplete_gamma_simplification}) et l'identité suivante (voir\cite{NIST:DLMF}): 
\[\Gamma(s+1, x)=s\Gamma(s, x)+x^s e^{-x}\]
il est possible de réécrire la solution sous la forme: 
\[
u(x)=C_1x^{-\beta} e^{\alpha x} -\theta\alpha^{-\beta-1}\left(\beta x^{-\beta}e^{\alpha x}\Gamma(\beta)\left(1-{(\alpha x)}^\beta e^{-\alpha x}\sum_{k=0}^{\infty} \frac{{(\alpha x)}^k}{\Gamma(\beta+k+1)}\right)+1\right)
\]

\paragraph{Intégration}\phantom{}\\
Pour obtenir la forme explicite de $A(x)$, il suffit d'intégrer $u(x)$ et d'ajouter une deuxième constante:
\[
\begin{aligned}
    A(x) &= \int u(x)dx+C_2 \\
    &= (C_1-\theta\alpha^{-\beta-1}\Gamma(\beta+1))\underbrace{\int x^{-\beta}e^{\alpha x}dx}_I +\frac{\theta\beta\Gamma(\beta)}{\alpha}\underbrace{\int\sum_{k=0}^{\infty} \frac{{(\alpha x)}^k}{\Gamma(\beta+k+1)}dx}_J-\theta\alpha^{-\beta-1}x +C_2 \\
\end{aligned}
\]
Les deux intégrales résolues précédemment $I$ et $J$ réapparaissent. En injectant leurs solutions, la forme finale de l'expression de la fonction Aire Moyenne est obtenue: 
\begin{equation}\label{sol_area}
    \begin{aligned}
            A(x) = {(-\alpha)}^{\beta-1}\Gamma(1-\beta,-\alpha x)[\theta\alpha^{-\beta-1}\Gamma(\beta+1)-C_1]+ \\ \frac{x\theta}{\alpha}\left[{}_2F_2\left(\begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}2\\1+\beta\end{bmatrix},\alpha x\right)-\alpha^{-\beta}\right] +C_2 
    \end{aligned}
\end{equation}


\paragraph{Détermination des constantes}\phantom{}\\
Les conditions aux limites $A(0)=A(c)=0$ nous permettent de déterminer les deux constantes $C_1$ et $C_2$ en résolvant le système suivant: 
\begin{align*}
\left\{\begin{aligned}
    {(-\alpha)}^{\beta-1}\Gamma(1-\beta)(\theta\alpha^{-\beta-1}\Gamma(\beta+1)-C_1)+C_2 &= 0 \\
    {(-\alpha)}^{\beta-1}\Gamma(1-\beta,-\alpha c)[\theta\alpha^{-\beta-1}\Gamma(\beta+1)-C_1]+\\ \frac{c\theta}{\alpha}\left[{}_2F_2\left(\begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}2\\1+\beta\end{bmatrix},\alpha c\right)-\alpha^{-\beta}\right] +C_2 &=0
\end{aligned}\right.
\end{align*}
Les constantes $C_1$ et $C_2$ s'écrivent donc: 
\begin{equation}\label{area_constants}
    \begin{aligned}
        C_1 =& \frac{1}{\gamma (1-\beta ,-\alpha c)}\Bigg[\theta  {(-\alpha)}^{-\beta } \alpha ^{-\beta -1} \bigg(c \alpha ^{\beta +1} \,{}_2F_2\left(\begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}2\\1+\beta\end{bmatrix},\alpha c\right)\\&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad+
        {(-\alpha)}^{\beta } \Gamma(\beta +1)\gamma (1-\beta ,-c \alpha )-\alpha  c\bigg)\Bigg] \\
        C_2 =& -\frac{c \theta  \alpha ^{-\beta -1} \Gamma (1-\beta ) }{\gamma(1-\beta ,-c \alpha )}\left[\alpha ^{\beta } \,{}_2F_2\left(\begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}2\\1+\beta\end{bmatrix},\alpha c\right)-1\right]
    \end{aligned}
\end{equation}

\subsection{Commande Optimale Stochastique}
Cette section est consacrée à l'étude du problème de commande optimale défini en (\ref{definition_optimal_control}).
\paragraph{Dérivation de l'équation de programmation dynamique}\phantom{}\\
En reprenant la définition (\ref{value_function}) et en appliquant le principe d'optimalité de Bellman, il découle:
\begin{equation}\label{Bellman_optimality}
        F(x) = \inf_u\mathds{E}\left[\int_0^{\delta t}\left(\frac{1}{2}q\left[X_u(s)\right]u^2\left[X_u(s)\right]+r\left[X_u(s)\right]\right)ds+F(X_u(\delta t))\right]
\end{equation}
D'une part, pour $\delta t$ petit, il est possible d'écrire:
\begin{equation}\label{cost_function_simplification}
    \int_0^{\delta t}\left(\frac{1}{2}q\left[X_u(s)\right]u^2\left[X_u(s)\right]+r\left[X_u(s)\right]\right)ds=\left(\frac{1}{2}q(x){u(x)}^2+r(x)\right)\delta t+o(\delta t)
\end{equation}
D'autre part, un développement de Taylor combiné avec la formule d'Itô (appliquée au processus contrôlé de dynamique (\ref{controlled_process})) permet d'écrire (en supposant que $F\in C^2$):
\begin{equation}\label{taylor_ito}
    \begin{aligned}
        \mathds{E}[F(X_u(\delta t))]&=F(x)+\mathds{E}[dF(X_u(\delta t))]+o(\delta t)\\
        &=F(x)+\mathds{E}\left[F'(X_u(\delta t))dX_u(t)+\frac{1}{2}F''(X_u(\delta t))d\langle X_u\rangle_{\delta t} \right]+o(\delta t)\\
        &=F(x)+\left[a(b-x)+b(x)u(x)\right]F'(x)\delta t+\frac{1}{2}\sigma^2xF''(x)\delta t+o(\delta t)
    \end{aligned}
\end{equation}
En injectant (\ref{cost_function_simplification}) et (\ref{taylor_ito}) dans (\ref{Bellman_optimality}), il découle:
\begin{equation}\label{initial_minimisation problem}
    \begin{aligned}
        F(x)&=\inf_u\Bigg\{\left(\frac{1}{2}q(x){u(x)}^2+r(x)\right)\delta t+F(x)+\left[a(b-x)+b(x)u(x)\right]F'(x)\delta t\\&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad+\frac{1}{2}\sigma^2xF''(x)\delta t+o(\delta t)\Bigg\}
    \end{aligned}
\end{equation}
Ensuite, en retranchant $F(x)$ des deux côtés, en divisant partout par $\delta t$ et en prenant la limite lorsque $\delta t\to0$, l'équation de programmation dynamique est obtenue:
\begin{equation}\label{minimisation_problem}
    0=\min_u\left\{r(x)+\frac{1}{2}q(x)u^2(x)+[a(b-x)+b(x)u(x)]F'(x)+\frac{1}{2}\sigma^2xF''(x)\right\}
\end{equation}
\paragraph{Détermination de la commande optimale et dérivation de l'équation associée}\phantom{}\\
La minimisation à faire dans (\ref{minimisation_problem}) est en fonction de $u(x)$. Le terme à minimiser est:
\[
f(u(x))=\frac{1}{2}q(x){u(x)}^2+b(x)u(x)F'(x)
\]
La minimisation donne:
\begin{equation}\label{optimal_control}
    \begin{aligned}
        f'(u^*(x))&=0\\
        q(x)u^*(x)+b(x)F'(x)&=0\\
    u^*(x)&=-\frac{b(x)}{q(x)}F'(x)
    \end{aligned}
\end{equation}
Par ailleurs, la dérivée seconde est: 
\[
f''(u(x))=q(x)\quad\forall\;u(x)
\]
Puisque \( q(x) > 0 \) par hypothèse (\ref{definition_optimal_control}), la dérivée seconde par rapport au contrôle est strictement positive. L'expression de \( u^*(x) \) obtenue en (\ref{optimal_control}) réalise donc bien un minimum global. Il s'agit ainsi du contrôle optimal.

En injectant ce dernier (\ref{optimal_control}) dans (\ref{minimisation_problem}), l'équation \acl{HJB} est obtenue, régissant la fonction valeur associée à la commande optimale:
\begin{equation}\label{control_equation}
    r(x) - \frac{{b(x)}^2}{2q(x)}{\left[F'(x)\right]}^2 + a(b - x)F'(x) + \frac{1}{2}\sigma^2 x F''(x) = 0
\end{equation}
avec $F(0)=F(c)=0$ et $r(x)\neq0, b(x)\neq0, q(x)>0$.


\subsubsection{Problème linéarisable}

Il faut donc procéder à la résolution de l'équation (\ref{control_equation}). Whittle (voir\cite{whittle1982}) a montré que la transformation: 
\[F(x)=-K\log(\varphi(x))\]
avec $\varphi(0)=\varphi(c)=1$ permet de linéariser l'équation en posant:
\[
K=\frac{\sigma^2xq(x)}{{b(x)}^2}
\]
L'équation devient:
\begin{equation}\label{linearised_equation}
    \frac{1}{2}\sigma^2 x\varphi''(x) + a(b - x)\varphi'(x) - \frac{r(x)}{K}\varphi(x) = 0
\end{equation}
Dans le cas où le terme suivant est constant:
\begin{equation}\label{condition}
    \frac{r(x)}{K}\equiv k\in\mathds{R}\;\forall\;x
\end{equation}
l'équation (\ref{linearised_equation}) est identique à celle de la \acl{FGM} $M(x;\alpha)$ pour:
\[
\alpha=\frac{r(x)}{K}
\]
Cela permet de résoudre une multitude de problèmes en supposant différentes formes pour $r(x)$, $b(x)$ et $q(x)$ tout en satisfaisant la condition (\ref{condition}). 
Deux exemples de problèmes sont donnés dans les sous-parties suivantes.

\paragraph{Problème 1}\label{p1}\phantom{}\\
\noindent Soit le problème suivant: 
\begin{itemize}
    \item $r(x) = \rho$: coût immédiat constant, indépendant de l'état $x$ et du contrôle.
    \item $b(x) = \beta x$: coût du contrôle proportionnel à $x$ sur la dynamique.
    \item $q(x) = \kappa x$: poids pénalisant l'intensité du contrôle, linéaire avec $x$.
\end{itemize}

Cela donne:
\[
K=\frac{\kappa \sigma^2}{\beta^2}
\]
L'équation devient:
\begin{equation}\label{eq_exemple1}
    \frac{1}{2}\sigma^2 x\varphi''(x) + a(b - x)\varphi'(x) - \frac{\rho\beta^2}{\kappa \sigma^2}\varphi(x) = 0
\end{equation}
La solution est:
\[
\varphi(x)=C_1\Phi\left(\frac{\rho\beta^2}{a\kappa \sigma^2},\frac{2ab}{\sigma^2},\frac{2ax}{\sigma^2}\right) + C_2\Psi\left(\frac{\rho\beta^2}{a\kappa \sigma^2},\frac{2ab}{\sigma^2},\frac{2ax}{\sigma^2}\right)
\]
Les conditions aux limites $\varphi(0)=\varphi(1)=1$ permettent de déterminer les constantes $C_1$ et $C_2$:
\begin{equation}\label{control_constants}
    \begin{aligned}
        C_1=\frac{\Psi\left(\frac{\beta ^2 \rho }{a \kappa  \sigma ^2},\frac{2 a b}{\sigma ^2},0\right)-\Psi\left(\frac{\beta ^2 \rho }{a \kappa  \sigma ^2},\frac{2 a b}{\sigma ^2},\frac{2 a c}{\sigma ^2}\right)}{\Psi\left(\frac{\beta ^2 \rho }{a \kappa  \sigma ^2},\frac{2 a b}{\sigma ^2},0\right) \, \Phi\left(\frac{\beta ^2 \rho }{a \kappa  \sigma ^2};\frac{2 a b}{\sigma ^2};\frac{2 a c}{\sigma ^2}\right)-\Psi\left(\frac{\beta ^2 \rho }{a \kappa  \sigma ^2},\frac{2 a b}{\sigma ^2},\frac{2 a c}{\sigma ^2}\right)} \\
        C_2= \frac{\, \Phi\left(\frac{\beta ^2 \rho }{a \kappa  \sigma ^2};\frac{2 a b}{\sigma ^2};\frac{2 a c}{\sigma ^2}\right)-1}{\Psi\left(\frac{\beta ^2 \rho }{a \kappa  \sigma ^2},\frac{2 a b}{\sigma ^2},0\right) \, \Phi\left(\frac{\beta ^2 \rho }{a \kappa  \sigma ^2};\frac{2 a b}{\sigma ^2};\frac{2 a c}{\sigma ^2}\right)-\Psi\left(\frac{\beta ^2 \rho }{a \kappa  \sigma ^2},\frac{2 a b}{\sigma ^2},\frac{2 a c}{\sigma ^2}\right)}
    \end{aligned}
\end{equation}
L'expression analytique de la fonction valeur est donc:
\begin{equation}\label{sol_control_1}
    F(x)=-\frac{\kappa \sigma^2}{\beta^2} \log\left[C_1\Phi\left(\frac{\rho\beta^2}{a\kappa \sigma^2},\frac{2ab}{\sigma^2},\frac{2ax}{\sigma^2}\right) + C_2\Psi\left(\frac{\rho\beta^2}{a\kappa \sigma^2},\frac{2ab}{\sigma^2},\frac{2ax}{\sigma^2}\right)\right]
\end{equation}
Par ailleurs, le contrôle optimal est
\begin{equation}\label{optimal_control_1}
    u^*(x)=-\frac{\beta}{\kappa}F'(x)
\end{equation}

\paragraph{Problème 2}\label{p2}\phantom{}\\
\noindent Soit le problème suivant: 
\begin{itemize}
    \item $r(x) = \rho x$: coût immédiat linéaire en $x$, indépendant du contrôle.
    \item $b(x) = \beta \sqrt{x}$: coût du contrôle proportionnel à $\sqrt{x}$.
    \item $q(x) = \kappa x$: poids pénalisant l'intensité du contrôle, linéaire avec $x$.
\end{itemize}
Cela donne: 
\[
K=\frac{x\kappa \sigma^2}{\beta^2}
\]
L'équation obtenue est identique à celle du premier problème (\ref{eq_exemple1}): 
\[
\frac{1}{2}\sigma^2 x\varphi''(x) + a(b - x)\varphi'(x) - \frac{\rho\beta^2}{\kappa \sigma^2}\varphi(x) = 0
\]
L'expression de $\varphi(x)$ est identique à celle trouvée précédemment. Il en découle donc l'expression de la solution $F(x)$: 
\begin{equation}\label{sol_control_2}
    F(x)=-\frac{x\kappa \sigma^2}{\beta^2}\left[C_1\Phi\left(\frac{\rho\beta^2}{a\kappa \sigma^2},\frac{2ab}{\sigma^2},\frac{2ax}{\sigma^2}\right) + C_2\Psi\left(\frac{\rho\beta^2}{a\kappa \sigma^2},\frac{2ab}{\sigma^2},\frac{2ax}{\sigma^2}\right)\right]
\end{equation}

Par ailleurs, le contrôle optimal est
\begin{equation}\label{optimal_control_2}
    u^*(x)=-\frac{\beta}{\kappa\sqrt{x}}F'(x)
\end{equation}

\subsubsection{Problème non linéarisable}\label{p3}
Soit le problème suivant:
\begin{itemize}
    \item $r(x) = x^2$: coût immédiat quadratique en $x$, indépendant du contrôle.
    \item $b(x) = x$: coût du contrôle linéaire égal à $x$.
    \item $q(x) \equiv 1$: poids pénalisant l'intensité du contrôle constant.
\end{itemize}
En éliminant le retour à la moyenne ($a=0$) et en posant $\sigma=1$, l'équation (\ref{control_equation}) se réduit à:
\begin{equation}\label{reduced_control_equation}
    x^2-\frac{x^2}{2}{F'(x)}^2+\frac{1}{2}xF''(x)=0
\end{equation}
Le logiciel de calcul symbolique \textit{Maple} donne comme solution pour $F(0)=0$ (avec $C_1$ une constante à déterminer):
\[
    F(x)=\int_0^x-\sqrt{2}\tanh\left(\frac{\sqrt{2}z^2}{2}+C_1\sqrt{2}\right)dz
\]
En posant $c=1$, la valeur de $C_1$ qui satisfait $F(c)=F(1)=0$ est $C_1\simeq -0.1652$. Donc, la forme finale de la fonction valeur est obtenue:
\begin{equation}\label{sol_control_3}
    F(x)\simeq \int_0^x-\sqrt{2}\tanh\left(\frac{\sqrt{2}z^2}{2}-0.1652\sqrt{2}\right)dz
\end{equation}
La commande optimale est donc:
\begin{equation}\label{optimal_control_3}
    u^*(x)\simeq x\sqrt{2}\tanh\left(\frac{\sqrt{2}x^2}{2}-0.1652\sqrt{2}\right)
\end{equation}

\section{Diffusion avec sauts}
Pour la suite, la variante discontinue du \acs{CIR} est portée à l'étude. Le processus est défini comme suit:
\begin{equation}\label{jump_cir_sde}
    X(t)=X(0)+\int_0^t a(b-X(s))ds+\int_0^t\sigma\sqrt{X(s)}dW(s)+J(t)
\end{equation}
avec
\begin{itemize}
    \item $X(0)=x$;
    \item $W(t)$ un \acs{MBS};
    \item $J(t)$ un processus de sauts pur 
    \[
    J(t)=\sum_{i=1}^{N(t)}Y_i
    \]
    où
    \begin{itemize}
        \item $N(t)$ est un processus de poisson de paramètre $\lambda$;
        \item $Y_i$ des variables indépendantes et distribuées suivant une certaine loi.
    \end{itemize}
\end{itemize}

\subsection{Fonction Temps moyen -- Sauts uniformes}\label{subsection_mean_jumps}
Dans cette sous-section, la variante du \ac{CIR} avec sauts considérée est celle avec des sauts négatifs modélisés par des variables uniformément distribuées $Y_i\sim U[-x,0]$.

\paragraph{Dérivation de l'équation à résoudre}\phantom{}\\
En reprenant ce qui avait été fait en (\ref{subsection_fgm_eq}) pour dériver l'\acs{EDO} régissant la \acl{FGM} de $\tau(x)$ (voir, par exemple,\cite{cox2017} ou\cite{lefebvre2007}), il est possible d'écrire:
\[
\frac{1}{2}\sigma^2 xM''(x;\alpha)+a(b-x)M'(x;\alpha)+\lambda\left\{\frac{1}{x}\int_{-x}^0M(x+y;\alpha)dy-M(x;\alpha)\right\}-\alpha M(x;\alpha)=0
\]
avec $M(0;\alpha)=M(c;\alpha)=1$.

Ensuite, en procédant comme dans (\ref{subsection_mean_eq}), il découle l'équation du temps moyen de sortie de l'intervalle pour le processus avec sauts:
\begin{equation}\label{mean_ide}
    \frac{1}{2}\sigma^2 xm''(x)+a(b-x)m'(x)+\lambda\left\{\frac{1}{x}\int_{-x}^0m(x+y)dy-m(x)\right\}=-1
\end{equation}
avec $m(0)=m(c)=0$.

Soit le changement de variable suivant:
\[
\int_{-x}^0m(x+y)dy=\int_0^x m(z)dz
\]
La formule de Leibniz permet d'écrire:
\[
\frac{d}{dx}\left(\int_0^x m(z)dz\right)=m(x)
\]
Donc, en dérivant les deux côtés de l'équation (\ref{mean_ide}) et en éliminant le retour à la moyenne ($a=0$), il découle une \acs{EDO} d'ordre 3:
\begin{equation}\label{mean_3rd_order}
    \frac{1}{2}\sigma^2xm'''(x)+\sigma^2m''(x)-\lambda m'(x)=-\frac{1}{x}
\end{equation}

\paragraph{Résolution}\phantom{}\\
Soit les valeurs suivantes des paramètres: $\sigma=\sqrt{2}$, $\lambda=1$ et $c=1$. \textit{Maple} donne la solution suivante:
\begin{equation}\label{sol_mean_with_jumps}
    m(x)=C_1I_0(2\sqrt{x})+C_2K_0(2\sqrt{x})+2\ln(2\sqrt{x})+C_3
\end{equation}
avec $C_1$, $C_2$, $C_3$ des constantes à déterminer et $I_0(\cdot)$, $K_0(\cdot)$ les fonctions de Bessel modifiées (voir annexe~\ref{special_functions}).

Les constantes $C_1$, $C_2$ et $C_3$ sont déterminées en imposant les conditions aux limites $m(0)=m(1)=0$ et $m(0.5)=r$. Ensuite, la valeur de $r$ permettant de satisfaire l'équation originale (\ref{mean_ide}) est trouvée: $r\simeq0.3281$.

\paragraph{Étude du cas sans sauts}\phantom{}\\
Afin de comparer l'effet de la présence des sauts, il est intéressant de résoudre le même problème en retirant ces derniers. Soit $m_0(x)$ le temps moyen de sortie du processus sans sauts. En considérant les mêmes valeurs des paramètres, l'équation à résoudre devient:
\begin{equation}\label{mean_3rd_order_without_jumps}
    xm_0''(x)=-1
\end{equation}
La solution qui satisfait $m_0(0)=m_0(1)=0$ est:
\begin{equation}\label{sol_mean_with_0_jumps}
    m_0(x)=-x\ln(x)
\end{equation}

\subsection{Fonction Probabilité de sortie en zéro -- Sauts uniformes}\label{subsection_probability_jumps}
Dans cette sous-section, la variante du \ac{CIR} avec sauts considérée est identique à la précédente (sauts négatifs modélisés par des variables uniformément distribuées $Y_i\sim U[-x,0]$).

\paragraph{Dérivation de l'équation à résoudre}\phantom{}\\
En procédant comme ce qui précède, il est possible d'écrire:
\begin{equation}
    \frac{1}{2}\sigma^2xp''(x)+a(b-x)p'(x)+\lambda\left\{\frac{1}{x}\int_{-x}^0p(x+y)dy-p(x)\right\}=0
\end{equation}
sous les conditions $p(0)=1$ et $p(1)=0$.

Ensuite, en effectuant le même changement de variable, en dérivant les deux membres de l'équation et en éliminant le retour à la moyenne ($a=0$), il découle:
\begin{equation}\label{probability_3rd_order}
    \frac{1}{2}\sigma^2xp'''(x)+\sigma^2p''(x)-\lambda p'(x)=0
\end{equation}
\paragraph{Résolution}\phantom{}\\
En reprenant les mêmes valeurs des paramètres ($\sigma=\sqrt{2}$, $\lambda=1$ et $c=1$) et en imposant les conditions $p(0)=1$, $p(1)=0$ et $p(0.5)=r$, \textit{Maple} donne la solution suivante:
\begin{equation}\label{sol_probability_with_jumps}
    p(x)=\frac{I_0(2)-I_0(2\sqrt{x})}{I_0(2)-1}
\end{equation}
avec $I_0(\cdot)$ la fonction de Bessel modifiée de première espèce (voir annexe~\ref{special_functions}).
L'expression obtenue (\ref{sol_probability_with_jumps}) est valide si $r\simeq0.5567$.
\paragraph{Étude du cas sans sauts}\phantom{}\\
Dans la même logique, le même problème en absence des sauts est résolu pour $p_0(x)=\mathds{P}[X(\tau(x))=0]$. L'équation (\ref{probability_3rd_order}) devient:
\[
xp_0''(x)=0
\]
La solution qui satisfait $p(0)=1$ et $p(1)=0$ est:
\begin{equation}\label{sol_probability}
    p_0(x)=1-x
\end{equation}

\subsection{Fonction Dépassement Moyen -- Sauts exponentiels}
Dans cette sous-section, la variante du \ac{CIR} avec sauts considérée est celle avec des sauts positifs modélisés par des variables aléatoires exponentielles $Y_i\sim \text{Exp}(\nu)$.

\paragraph{Dérivation de l'équation à résoudre}\phantom{}\\
Soit $f(x)=(x-c)\mathds{1}_{x\geq c}$ la fonction mesurant un dépassement. En appliquant la formule de Dynkin (voir\cite{dynkin1965}), il est possible d'écrire:
\begin{equation}\label{initial_dynkin}
    \mathds{E}[f(X(\tau(x)))]=f(x)+\mathds{E}\left[\int_0^{\tau(x)}\mathcal{L}f(X(s))ds\right]
\end{equation}
D'abord, en développant le terme de gauche, l'expression de la fonction $D(x)$ définie en (\ref{overshoot}) est retrouvée:
\begin{equation}\label{dynkin_left_simplification}
    \begin{aligned}
        \mathds{E}[f(X(\tau(x)))]&=\mathds{E}\left[(X(\tau(x))-c)\mathds{1}_{X(\tau(x))\geq c}\right]\\
        &=D(x)
    \end{aligned}
\end{equation}
Ensuite, le terme de droite peut être simplifié: 
\begin{equation}\label{dynkin_right_simplification_1}
    X(0)=x\in(0,c)\implies f(x)=0
\end{equation}
Par ailleurs, en développant le générateur infinitésimal (voir annexe \ref{infinitesimal_generator}) appliqué à la fonction $f(x)$, il découle que:
\begin{equation}\label{dynkin_right_simplification_2}
    \begin{aligned}
        \mathcal{L}f(x)&=\frac{1}{2}\sigma^2xf''(x)+a(b-x)f'(x)+\lambda\int_0^{+\infty}\left[f(x+y)-f(x)\right]\nu e^{-\nu y}dy \\
        &=\lambda\int_{c-x}^{+\infty}(x+y-c)\nu e^{-\nu y}dy \\
        &=\frac{\lambda}{\nu e^{\nu c}}e^{\nu x}
    \end{aligned}
\end{equation}
En prenant en compte (\ref{dynkin_left_simplification},\ref{dynkin_right_simplification_1},\ref{dynkin_right_simplification_2}), l'équation (\ref{initial_dynkin}) devient:
\begin{equation}\label{simplified_dynkin}
    D(x)=\frac{\lambda}{\nu e^{\nu c}}\mathds{E}\left[\int_0^{\tau(x)}e^{\nu X(s)}ds\right]
\end{equation}
Soit la fonction $g(x)$ définie par:
\begin{equation}\label{g_defintion}
    g(x)=\mathds{E}\left[\int_0^{\tau(x)}e^{\nu X(s)}ds\right]
\end{equation}
Comme les coefficients de dérive et de diffusion du processus \acs{CIR} vérifient les conditions d'unicité trajectorielle (existence et unicité forte de la solution de l'équation \ref{cir_eq}), le travail présenté dans\cite{abundo2013} peut être utilisé pour établir l'\acs{EID} du second ordre associée:
\begin{equation}\label{initial_overshoot_ide}
    \frac{1}{2}\sigma^2xg''(x)+a(b-x)g'(x)+\lambda\int_0^{+\infty}\left[g(x+y)-g(x)\right]\nu e^{-\nu y}dy=-e^{\nu x}
\end{equation}
En séparant l'intégrale, il découle:
\begin{equation}\label{overshoot_ide}
        \frac{1}{2}\sigma^2xg''(x)+a(b-x)g'(x)+\lambda\int_0^{+\infty}g(x+y)\nu e^{-\nu y}dy-\lambda g(x)=-e^{\nu x}
\end{equation}
Le changement de variable $z=x+y$ permet d'écrire:
\[
\int_0^{+\infty}g(x+y)\nu e^{-\nu y}dy=\nu e^{\nu x}\int_x^{+\infty}g(z)e^{-\nu z}dz
\]
Soit:
\begin{equation}\label{phi_simplification}
    \Phi(x):=\int_x^{+\infty}g(z)e^{-\nu z}dz
\end{equation}
La formule de Leibniz donne:
\begin{equation}\label{phi_derivation}
        \Phi'(x)=-g(x)e^{-\nu x}\implies g(x)=-e^{\nu x}\Phi'(x)
\end{equation}
En injectant (\ref{phi_simplification},\ref{phi_derivation}) dans (\ref{overshoot_ide}), l'équation devient:
\begin{equation}\label{Phi_equation}
    \begin{aligned}
        \frac{1}{2}\sigma^2x\Phi'''(x)+\Phi''(x)\left[\nu\sigma^2x+a(b-x)\right]\\+\Phi'(x)\left[\frac{1}{2}\sigma^2\nu^2x+a\nu(b-x)-\lambda\right]-\lambda\nu \Phi(x)=1
    \end{aligned}
\end{equation}
Le fonction recherchée $g(x)$ ne dépend que de $\Phi'(x)$. En posant $\phi(x)=\Phi'(x)$ et en dérivant l'équation (\ref{Phi_equation}), il découle l'\acs{EDO} homogène linéaire d'ordre 3:
\begin{equation}\label{phi_equation}
    \begin{aligned}
        x \phi'''(x)+\phi''(x) \left[\left(\frac{2(\nu\sigma^2-a)}{\sigma^2}\right)x+\left(\frac{2ab+\sigma^2}{\sigma^2}\right)\right]\\+\phi'(x) \left[\left(\frac{\sigma^2\nu^2-2a\nu}{\sigma^2}\right)x+\left(\frac{2(\nu\sigma^2-a-\lambda+ab\nu)}{\sigma^2}\right)\right]\\+\phi(x) \left[\frac{\nu^2  \sigma ^2-2\nu(a+\lambda )}{\sigma^2}\right]=0
    \end{aligned}
\end{equation}
Les conditions $g(0)=g(c)=0$ deviennent $\phi(0)=\phi(c)=0$. Enfin
\[
\begin{aligned}
    D(x) &= \frac{\lambda}{\nu e^{\nu c}}g(x)\\
    &=-\frac{\lambda}{\nu e^{\nu c}}e^{\nu x}\phi(x)
\end{aligned}
\]

\paragraph{Résolution approximative pour $a=0$}\phantom{}\\
En remplaçant $g(x+y)$ dans (\ref{initial_overshoot_ide}) par un développement de Taylor, il est possible de réécrire l'intégrale comme suit:
\[
\begin{aligned}
    \lambda\int_0^{+\infty}\left[g(x+y)-g(x)\right]\nu e^{-\nu y}dy&=\lambda\int_0^{+\infty}\left[\sum_{n=0}^{+\infty}\frac{y^n}{n!}\frac{d^n}{dx^n}g(x)-g(x)\right]\nu e^{-\nu y}dy\\
    &=\lambda\int_0^{+\infty}\left[\sum_{n=1}^{+\infty}\frac{y^n}{n!}\frac{d^n}{dx^n}g(x)\right]\nu e^{-\nu y}dy
\end{aligned}
\]
Ensuite, en échangeant la série et l'intégrale, le $n$-ème moment de $Y_i$ apparaît, permettant ainsi de simplifier l'expression:
\[
\begin{aligned}
    \lambda\int_0^{+\infty}\left[\sum_{n=1}^{+\infty}\frac{y^n}{n!}\frac{d^n}{dx^n}g(x)\right]\nu e^{-\nu y}dy&=\lambda\sum_{n=0}^{+\infty}\left[\frac{1}{n!}\frac{d^n}{dx^n}g(x)\int_0^{+\infty}y^n\nu e^{-\nu y}dy\right]\\
    &=\lambda\sum_{n=0}^{+\infty}\frac{1}{n!}\frac{d^n}{dx^n}g(x)\mathds{E}\left[Y_i^n\right]\\
    &=\lambda\sum_{n=0}^{+\infty}\frac{1}{\nu^n}\frac{d^n}{dx^n}g(x)
\end{aligned}
\]
Par ailleurs, en supposant que les sauts sont petits ($\nu$ est grand), il est possible d'approximer la somme obtenue de la façon suivante:
\[
\lambda\sum_{n=0}^{+\infty}\frac{1}{\nu^n}\frac{d^n}{dx^n}g(x)\approx\frac{1}{\nu}g'(x)+\frac{1}{\nu^2}g''(x)
\]
L'équation (\ref{initial_overshoot_ide}) devient:
\begin{equation}\label{final_overshoot_ode}
    \left(\frac{1}{2}\sigma^2x+\frac{\lambda}{\nu^2}\right)g''(x)+\left[a(b-x)+\frac{\lambda}{\nu}\right]g'(x) =-e^{\nu x}
\end{equation}
Cette \acs{EDO} de second ordre est non homogène avec des coefficients polynomiaux. La solution explicite, si elle existe, est difficile à obtenir. Dans la suite, un cas particulier du problème est considéré.

En fixant le paramètre $a$ à 0, cela revient à éliminer le mécanisme de \textit{retour à la moyenne} présenté en (\ref{cir_eq}). Dans ces conditions, l'\acs{EDO} (\ref{final_overshoot_ode}) devient:
\begin{equation}\label{particular_overshoot_ode}
    \left(\frac{1}{2}\sigma^2x+\frac{\lambda}{\nu^2}\right)g''(x)+\frac{\lambda}{\nu}g'(x) =-e^{\nu x}
\end{equation}
\textit{Wolfram Mathematica} donne:
\[
\begin{aligned}
    g(x)=\frac{1}{{\nu\sigma^2(2\lambda-\nu\sigma^2)}}\Bigg[\sigma^2\left(-C_1 \left(2 \lambda +\nu ^2 \sigma ^2 x\right)^{1-\frac{2 \lambda }{\nu  \sigma ^2}}+C_2 \nu  \left(2 \lambda -\nu  \sigma ^2\right)-2 \nu  e^{\nu  x}\right) \\-2 e^{-\frac{2 \lambda }{\nu  \sigma ^2}} \left(2 \lambda +\nu ^2 \sigma ^2 x\right) E_{1-\frac{2 \lambda }{\nu  \sigma ^2}}\left(-\frac{2 \lambda }{\nu  \sigma ^2}-x \nu \right)\Bigg]
\end{aligned}
\]
avec $E_n(x)$ la fonction intégrale exponentielle (voir annexe \ref{special_functions}). \\
La forme finale de l'expression de la fonction Dépassement Moyen est obtenue:
\begin{equation}\label{sol_overshoot}
    \begin{aligned}
            D(x)=\frac{\lambda}{{\nu^2\sigma^2e^{\nu c}(2\lambda-\nu\sigma^2)}}\Bigg[\sigma^2\left(-C_1 \left(2 \lambda +\nu ^2 \sigma ^2 x\right)^{1-\frac{2 \lambda }{\nu  \sigma ^2}}+C_2 \nu  \left(2 \lambda -\nu  \sigma ^2\right)-2 \nu  e^{\nu  x}\right) \\-2 e^{-\frac{2 \lambda }{\nu  \sigma ^2}} \left(2 \lambda +\nu ^2 \sigma ^2 x\right) E_{1-\frac{2 \lambda }{\nu  \sigma ^2}}\left(-\frac{2 \lambda }{\nu  \sigma ^2}-x \nu \right)\Bigg]
    \end{aligned}
\end{equation}

\paragraph{Détermination des constantes}\phantom{}\\
Les conditions aux limites $g(0)=g(c)=0$ (et donc $D(0)=D(c)=0$) permettent de déterminer les deux constantes $C_1$ et $C_2$. Étant donné la longueur des expressions obtenues, celles-ci ne seront pas détaillées dans le présent document.

